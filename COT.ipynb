{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Import your modules\n",
    "from src import *\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "import logging\n",
    "logging.basicConfig(filename=\"myfile.txt\",level=logging.DEBUG)\n",
    "!VLLM_CONFIGURE_LOGGING=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def builder(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    generator = GetPaths(args, \n",
    "                              topk=args.topk, \n",
    "                              max_new_tokens=args.max_new_tokens, )\n",
    "\n",
    "    cot_decoding = Decoder(tokenizer=tokenizer,)\n",
    "\n",
    "    return generator, cot_decoding, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#,  # \"microsoft/phi-2\" Set your desired model. Qwen/Qwen2.5-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 22:34:09 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-25 22:34:10 config.py:2448] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m\n\u001b[1;32m     41\u001b[0m strategies \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopp_sampling_0.9\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m}),\n\u001b[1;32m     43\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopp_sampling_0.8\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m}),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \n\u001b[1;32m     48\u001b[0m ]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize generator, CoT decoding, and tokenizer.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m generator, cot_decoding, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load the MultiArith dataset.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChilleD/MultiArith\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mbuilder\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(args\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m----> 4\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGetPaths\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m cot_decoding \u001b[38;5;241m=\u001b[39m Decoder(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generator, cot_decoding, tokenizer\n",
      "File \u001b[0;32m/workspace/src/COTdecoding.py:35\u001b[0m, in \u001b[0;36mGetPaths.__init__\u001b[0;34m(self, args, max_new_tokens, topk, stop, prompt)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, \n\u001b[1;32m     31\u001b[0m              max_new_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m, \n\u001b[1;32m     32\u001b[0m              topk: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     33\u001b[0m              stop: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExercise\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     34\u001b[0m              prompt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_new_tokens \u001b[38;5;241m=\u001b[39m max_new_tokens\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m stop\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:486\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/engine/arg_utils.py:1127\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU offload space must be non-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1126\u001b[0m device_config \u001b[38;5;241m=\u001b[39m DeviceConfig(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1127\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (model_config\u001b[38;5;241m.\u001b[39mis_multimodal_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mVLLM_USE_V1\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_prefix_caching):\n\u001b[1;32m   1131\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--enable-prefix-caching is currently not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for multimodal models in v0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas been disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/engine/arg_utils.py:1047\u001b[0m, in \u001b[0;36mEngineArgs.create_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model_config\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelConfig:\n\u001b[0;32m-> 1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We know this is not None because we set it in __post_init__\u001b[39;49;00m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce_eager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserved_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserved_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_async_output_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_async_output_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_mm_preprocessor_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_mm_preprocessor_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_neuron_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_neuron_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_pooler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_pooler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_processor_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_generation_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_generation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_sleep_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_sleep_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/config.py:366\u001b[0m, in \u001b[0;36mModelConfig.__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, allowed_local_media_path, revision, code_revision, rope_scaling, rope_theta, tokenizer_revision, max_model_len, spec_target_max_model_len, quantization, enforce_eager, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name, limit_mm_per_prompt, use_async_output_proc, config_format, hf_overrides, mm_processor_kwargs, disable_mm_preprocessor_cache, override_neuron_config, override_pooler_config, logits_processor_pattern, generation_config, enable_sleep_mode, override_generation_config, model_impl)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_model_len \u001b[38;5;241m=\u001b[39m _get_and_verify_max_len(\n\u001b[1;32m    358\u001b[0m     hf_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_text_config,\n\u001b[1;32m    359\u001b[0m     max_model_len\u001b[38;5;241m=\u001b[39mmax_model_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     spec_target_max_model_len\u001b[38;5;241m=\u001b[39mspec_target_max_model_len,\n\u001b[1;32m    363\u001b[0m     encoder_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_config)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserved_model_name \u001b[38;5;241m=\u001b[39m get_served_model_name(model,\n\u001b[1;32m    365\u001b[0m                                                served_model_name)\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodal_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_multimodal_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_tokenizer_init:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_tokenizer_mode()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/config.py:427\u001b[0m, in \u001b[0;36mModelConfig._init_multimodal_config\u001b[0;34m(self, limit_mm_per_prompt)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_multimodal_config\u001b[39m(\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m, limit_mm_per_prompt: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]\n\u001b[1;32m    425\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiModalConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    426\u001b[0m     architectures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchitectures\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mModelRegistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_multimodal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitectures\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiModalConfig(limit_per_prompt\u001b[38;5;241m=\u001b[39mlimit_mm_per_prompt \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m limit_mm_per_prompt:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:460\u001b[0m, in \u001b[0;36m_ModelRegistry.is_multimodal_model\u001b[0;34m(self, architectures)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_multimodal_model\u001b[39m(\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    458\u001b[0m     architectures: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     model_cls, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitectures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_cls\u001b[38;5;241m.\u001b[39msupports_multimodal\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:416\u001b[0m, in \u001b[0;36m_ModelRegistry.inspect_model_cls\u001b[0;34m(self, architectures)\u001b[0m\n\u001b[1;32m    413\u001b[0m architectures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_archs(architectures)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arch \u001b[38;5;129;01min\u001b[39;00m architectures:\n\u001b[0;32m--> 416\u001b[0m     model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_inspect_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43march\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (model_info, arch)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:391\u001b[0m, in \u001b[0;36m_ModelRegistry._try_inspect_model_cls\u001b[0;34m(self, model_arch)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_arch \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_try_inspect_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:319\u001b[0m, in \u001b[0;36m_try_inspect_model_cls\u001b[0;34m(model_arch, model)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_inspect_model_cls\u001b[39m(\n\u001b[1;32m    315\u001b[0m     model_arch: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    316\u001b[0m     model: _BaseRegisteredModel,\n\u001b[1;32m    317\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[_ModelInfo]:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in inspecting model architecture \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    322\u001b[0m                          model_arch)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:290\u001b[0m, in \u001b[0;36m_LazyRegisteredModel.inspect_model_cls\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minspect_model_cls\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ModelInfo:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_in_subprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ModelInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/registry.py:522\u001b[0m, in \u001b[0;36m_run_in_subprocess\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    518\u001b[0m input_bytes \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mdumps((fn, output_filepath))\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# cannot use `sys.executable __file__` here because the script\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# contains relative imports\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m returned \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_SUBPROCESS_COMMAND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# check if the subprocess is successful\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set up evaluation arguments.\n",
    "args = SimpleNamespace(\n",
    "    seed=42,\n",
    "    model= \"meta-llama/Llama-3.2-1B\", #\"meta-llama/Llama-3.2-1B\", ##\"google/gemma-2-2b\", #,  # Qwen/Qwen2.5-1.5B, meta-llama/Llama-3.2-3B\n",
    "    dtype=\"float16\",\n",
    "    max_model_len=512,\n",
    "    topk=10,                   # Number of paths to generate.\n",
    "    max_new_tokens=512,        # Maximum tokens to generate.\n",
    "    pattern=r'-?\\d+\\.?\\d*',    # Regex pattern for arithmetic answers.\n",
    "    demo_prompt=\"\",            # This will be updated for each sample.\n",
    "    groundtruth=\"\",            # This will be updated for each sample.\n",
    "    verbose=False               # Verbose flag to print each output.\n",
    ")\n",
    "\n",
    "# Define decoding strategies to test.\n",
    "# For sampling modes, we pass temperature and top_p via the continue_decoding method.\n",
    "strategies = [\n",
    "    (\"greedy\", {\"mode\": \"greedy\"}),\n",
    "    (\"beam\", {\"mode\": \"beam\", \"num_beams\": 5}),\n",
    "    (\"topk_sampling\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 1.0}),\n",
    "    (\"topp_sampling\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.9})\n",
    "]\n",
    "\n",
    "strategies = [\n",
    "    (\"topp_sampling_1\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.9}),\n",
    "    (\"topp_sampling_0.9\", {\"mode\": \"sampling\", \"temperature\": 0.9, \"top_p\": 0.9}),\n",
    "    (\"topp_sampling_0.7\", {\"mode\": \"sampling\", \"temperature\": 0.7, \"top_p\": 0.9}),\n",
    "    (\"topp_sampling_0.5\", {\"mode\": \"sampling\", \"temperature\": 0.5, \"top_p\": 0.9}),\n",
    "    \n",
    "]\n",
    "    \n",
    "strategies = [\n",
    "    (\"topp_sampling_0.9\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.9}),\n",
    "    (\"topp_sampling_0.8\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.8}),\n",
    "    (\"topp_sampling_0.7\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.7}),\n",
    "    (\"topp_sampling_0.6\", {\"mode\": \"sampling\", \"temperature\": 1.0, \"top_p\": 0.6}),\n",
    "    \n",
    "    \n",
    "]\n",
    "    \n",
    "\n",
    "# Initialize generator, CoT decoding, and tokenizer.\n",
    "generator, cot_decoding, tokenizer = builder(args)\n",
    "\n",
    "# Load the MultiArith dataset.\n",
    "dataset = load_dataset(\"ChilleD/MultiArith\", split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating strategy: topp_sampling_0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:   6%|▋         | 27/420 [00:44<09:27,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:08:59 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:   8%|▊         | 35/420 [01:05<11:17,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:09:19 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  12%|█▏        | 50/420 [01:52<28:23,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:10:07 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  21%|██        | 89/420 [02:48<06:29,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:11:02 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  40%|███▉      | 166/420 [04:47<05:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:13:01 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  46%|████▋     | 195/420 [05:40<11:24,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:13:55 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  63%|██████▎   | 264/420 [07:50<07:02,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:16:04 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  68%|██████▊   | 287/420 [08:26<02:47,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:16:41 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  81%|████████  | 340/420 [10:20<01:54,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:18:34 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  88%|████████▊ | 369/420 [11:11<01:42,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:19:26 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  90%|█████████ | 378/420 [11:30<00:44,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:19:44 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9:  94%|█████████▍| 394/420 [12:00<00:38,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:20:15 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.9: 100%|██████████| 420/420 [12:38<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy for strategy topp_sampling_0.9: 0.1738\n",
      "\n",
      "\n",
      "Evaluating strategy: topp_sampling_0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:   6%|▋         | 27/420 [00:43<09:38,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:21:37 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:   8%|▊         | 35/420 [01:03<09:09,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:21:56 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  12%|█▏        | 50/420 [01:50<28:23,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:22:43 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  21%|██        | 89/420 [02:42<06:16,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:23:36 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  40%|███▉      | 166/420 [04:35<05:09,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:25:29 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  46%|████▋     | 195/420 [05:29<11:49,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:26:23 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  63%|██████▎   | 264/420 [07:20<07:08,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:28:14 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  68%|██████▊   | 287/420 [07:56<02:45,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:28:50 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  81%|████████  | 340/420 [09:38<01:42,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:30:31 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  88%|████████▊ | 369/420 [10:28<01:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:31:22 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  90%|█████████ | 378/420 [10:47<00:44,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:31:40 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8:  94%|█████████▍| 394/420 [11:17<00:37,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:32:10 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.8: 100%|██████████| 420/420 [11:55<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy for strategy topp_sampling_0.8: 0.1667\n",
      "\n",
      "\n",
      "Evaluating strategy: topp_sampling_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:   6%|▋         | 27/420 [00:43<09:20,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:33:32 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:   8%|▊         | 35/420 [01:02<09:07,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:33:51 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  12%|█▏        | 50/420 [01:49<28:09,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:34:38 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  21%|██        | 89/420 [02:42<06:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:35:31 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  40%|███▉      | 166/420 [04:28<05:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:37:17 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  46%|████▋     | 195/420 [05:21<11:20,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:38:10 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  63%|██████▎   | 264/420 [07:12<06:58,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:40:01 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  68%|██████▊   | 287/420 [07:48<02:46,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:40:37 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  81%|████████  | 340/420 [09:26<01:42,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:42:15 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  88%|████████▊ | 369/420 [10:16<01:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:43:06 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  90%|█████████ | 378/420 [10:35<00:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:43:24 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7:  94%|█████████▍| 394/420 [11:05<00:37,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:43:54 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.7: 100%|██████████| 420/420 [11:43<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy for strategy topp_sampling_0.7: 0.1714\n",
      "\n",
      "\n",
      "Evaluating strategy: topp_sampling_0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:   6%|▋         | 27/420 [00:43<09:27,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:45:16 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:   8%|▊         | 35/420 [01:03<09:08,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:45:35 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  12%|█▏        | 50/420 [01:50<28:09,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:46:22 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  21%|██        | 89/420 [02:42<06:16,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:47:14 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  40%|███▉      | 166/420 [04:28<05:10,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:49:00 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  46%|████▋     | 195/420 [05:21<11:16,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:49:54 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  63%|██████▎   | 264/420 [07:11<07:01,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:51:44 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  68%|██████▊   | 287/420 [07:47<02:45,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:52:20 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  81%|████████  | 340/420 [09:25<01:41,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:53:57 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  88%|████████▊ | 369/420 [10:15<01:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:54:48 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  90%|█████████ | 378/420 [10:33<00:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:55:06 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6:  94%|█████████▍| 394/420 [11:04<00:37,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-25 12:55:36 scheduler.py:1091] Input prompt (513 tokens) is too long and exceeds limit of 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating topp_sampling_0.6: 100%|██████████| 420/420 [11:41<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy for strategy topp_sampling_0.6: 0.1667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each decoding strategy.\n",
    "with open('llama_petit.txt', 'w') as f:\n",
    "\n",
    "    for strategy_name, params in strategies:\n",
    "        print(f\"\\nEvaluating strategy: {strategy_name}\")\n",
    "        print(f\"\\nEvaluating strategy: {strategy_name}\", file=f)\n",
    "        correct_count = 0\n",
    "        total = 0\n",
    "\n",
    "        # Loop over the dataset with a progress bar.\n",
    "        for sample in tqdm(dataset, desc=f\"Evaluating {strategy_name}\"):\n",
    "            question = sample[\"question\"]\n",
    "            groundtruth = str(sample[\"final_ans\"]).strip()  # Convert and trim groundtruth.\n",
    "            \n",
    "            # Update args for current sample.\n",
    "            args.demo_prompt = question\n",
    "            args.groundtruth = groundtruth\n",
    "            \n",
    "            # --- Initial CoT generation (always using greedy for the first token) ---\n",
    "            topk_tokens, outputs = generator.search_cots(args.demo_prompt)\n",
    "            cot_paths = cot_decoding.get_score(args.demo_prompt, topk_tokens, outputs, groundtruth)\n",
    "            best_reasoning = cot_paths.best_path.reasoning_text\n",
    "            \n",
    "            # --- Continue decoding if not using greedy ---\n",
    "            if strategy_name != \"greedy\":\n",
    "                # Create a partial prompt (the Q/A format is preserved).\n",
    "                partial_prompt = generator.format_prompt(question) + best_reasoning\n",
    "                continued_outputs = generator.continue_decoding([partial_prompt], **params)\n",
    "                # Append the continued text to the initial best reasoning.\n",
    "                continued_text = continued_outputs[0].outputs[0].text\n",
    "                final_reasoning = best_reasoning + continued_text\n",
    "            else:\n",
    "                final_reasoning = best_reasoning\n",
    "            \n",
    "            # --- Extract answer using the regex pattern ---\n",
    "            extracted = re.findall(args.pattern, final_reasoning)\n",
    "            if extracted:\n",
    "                best_answer = extracted[-1].strip()\n",
    "            else:\n",
    "                best_answer = \"Did not respond explicitly\"\n",
    "            \n",
    "            # Check if the extracted answer matches the groundtruth.\n",
    "            is_correct = (best_answer == groundtruth)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            total += 1\n",
    "            \n",
    "            # Verbose output for each sample.\n",
    "            if args.verbose:\n",
    "                print(f\"\\nSample {total}:\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Groundtruth: {groundtruth}\")\n",
    "                print(f\"Strategy: {strategy_name}\")\n",
    "                print(f\"Predicted Answer: {best_answer}\")\n",
    "                print(f\"Correct: {is_correct}\\n\")\n",
    "        \n",
    "        # Calculate and print the average accuracy for the strategy.\n",
    "        accuracy = correct_count / total if total > 0 else 0\n",
    "        print(f\"Average Accuracy for strategy {strategy_name}: {accuracy:.4f}\\n\")\n",
    "        print(f\"Average Accuracy for strategy {strategy_name}: {accuracy:.4f}\\n\", file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_greedy_accuracy(generator, args, dataset):\n",
    "    correct_count = 0\n",
    "    total = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        question = sample[\"question\"]\n",
    "        # Groundtruth answer is expected to be a string (convert if needed)\n",
    "        groundtruth = str(sample[\"final_ans\"]).strip()\n",
    "        \n",
    "        # Format the prompt using the generator's formatting method.\n",
    "        formatted_prompt = generator.format_prompt(question)\n",
    "        \n",
    "        # Set up sampling parameters for greedy decoding.\n",
    "        sampling_params = SamplingParams(\n",
    "            n=1,\n",
    "            temperature=0,  # Greedy decoding\n",
    "            top_p=1,\n",
    "            max_tokens=args.max_new_tokens,\n",
    "            logprobs=2,\n",
    "        )\n",
    "        \n",
    "        # Generate a single output.\n",
    "        outputs = generator.model.generate(formatted_prompt, sampling_params, use_tqdm=False)\n",
    "        output = outputs[0]\n",
    "        generated_text = output.outputs[0].text\n",
    "        \n",
    "        # Extract the answer candidate using regex.\n",
    "        answer_candidates = re.findall(args.pattern, generated_text)\n",
    "        if answer_candidates:\n",
    "            # Use the last found candidate.\n",
    "            predicted_answer = answer_candidates[-1].strip()\n",
    "        else:\n",
    "            predicted_answer = \"\"\n",
    "        \n",
    "        # Compare the predicted answer to the groundtruth.\n",
    "        is_correct = (predicted_answer == groundtruth)\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        total += 1\n",
    "        \n",
    "        \"\"\"print(f\"Question: {question}\")\n",
    "        print(f\"Groundtruth: {groundtruth}\")\n",
    "        print(f\"Predicted: {predicted_answer}\")\n",
    "        print(f\"Correct: {is_correct}\\n\")\"\"\"\n",
    "    \n",
    "    accuracy = correct_count / total if total > 0 else 0\n",
    "    print(f\"Greedy Decoding Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding Accuracy: 0.0262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compute_greedy_accuracy(generator, args, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
