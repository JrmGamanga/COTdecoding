import re
import torch

from typing import List, Dict
from vllm import LLM, SamplingParams
from dataclasses import dataclass
from transformers import AutoTokenizer


# Define a dataclass to hold information about each generated reasoning path.
@dataclass
class Path:
    reasoning_text: str  # The complete reasoning generated by the model.
    score: float         # The computed score (e.g., based on token probabilities) for this path.
    answer_span: str     # The extracted answer segment from the reasoning text.
    num_path: int        # The index number of this reasoning path.
    is_correct: bool     # Boolean flag indicating if the extracted answer is correct (if groundtruth is provided).


# Define a dataclass to encapsulate the overall result.
@dataclass
class Result:
    question: str        # The original question or prompt.
    paths: List[Path]    # A list of all generated reasoning paths.
    best_path: Path      # The best path based on the computed score.


# Class for generating multiple reasoning paths from a given prompt.
class GetPaths():
    def __init__(self, args, 
                 max_new_tokens: int = 300, 
                 topk: int = 10, 
                 stop: List[str] = ['Q:', '\n\nQ:', '\n\nExercise'],
                 prompt: str = ''):
        self.model = LLM(model=args.model, seed=args.seed, dtype=args.dtype, max_model_len=args.max_model_len)
        self.max_new_tokens = max_new_tokens
        self.stop = stop
        self.topk = topk
        
        self.model.llm_engine.model_config.max_logprobs = self.topk + 1
        
        self.tokenizer = self.model.llm_engine.tokenizer.tokenizer
        self.prompt = prompt

    def search_cots(self, raw_prompt: str) -> List[str]:
        formatted_query = self.format_prompt(raw_prompt)
        first_tokens = self.get_first_topk_tokens(formatted_query)
        query_prompts = [formatted_query + token for token in first_tokens['decoded']]
        path_outputs = self.generate_paths(query_prompts)
        return first_tokens, path_outputs

    @torch.inference_mode()
    def get_first_topk_tokens(self, prompt: str) -> Dict[str, List]:
        sampling_settings = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=1, logprobs=self.topk, stop=self.stop)
        output_logs = self.model.generate(prompt, sampling_settings, use_tqdm=False)[0].outputs[0].logprobs[0]

        token_details = {'decoded': [], 'probs': [], 'token_id': [], 'logprobs': []}
        for token_identifier, lp_obj in output_logs.items():
            token_details['logprobs'].append({token_identifier: lp_obj})
            token_details['decoded'].append(lp_obj.decoded_token)
            token_details['probs'].append(lp_obj.logprob)
            token_details['token_id'].append(token_identifier)
        token_details['probs'] = torch.exp(torch.tensor(token_details['probs'])).tolist()
        return token_details

    @torch.inference_mode()
    def generate_paths(self, prompts: List[str]) -> Dict[int, Dict]:
        generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        generated_outputs = self.model.generate(prompts, generation_params, use_tqdm=False)
        return generated_outputs

    def continue_decoding(self, prompts: List[str], mode: str = "sampling", temperature: float = 1.0, top_p: float = 0.9, num_beams: int = None) -> Dict[int, Dict]:
        if mode == "sampling":
            generation_params = SamplingParams(n=1, temperature=temperature, top_p=top_p, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        elif mode == "beam":
            generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
            if num_beams is not None:
                generation_params.num_beams = num_beams
        else:
            generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        generated_outputs = self.model.generate(prompts, generation_params, use_tqdm=False)
        return generated_outputs

    def format_prompt(self, raw_prompt: str) -> str:
        return f'Q:{raw_prompt}\nA:{self.prompt}'


# New helper class to extract the answer span and compute score.
class AnswerExtractor:
    def __init__(self, tokenizer: AutoTokenizer, pattern: str = r'-?\d+\.?\d*'):
        self.tokenizer = tokenizer
        self.pattern = pattern

    def extract(self, full_reasoning: str, first_logprob: Dict, result_logprobs: List[Dict], groundtruth=None):
        tokenized = self.tokenizer(full_reasoning, return_offsets_mapping=True)
        answer_matches = re.findall(self.pattern, full_reasoning)
        if answer_matches:
            extracted = answer_matches[-1]
        else:
            return 'Did not respond explicitly', 0, False

        answer_range = (full_reasoning.rfind(extracted), full_reasoning.rfind(extracted) + len(extracted))
        answer_indices = [j for j, span in enumerate(tokenized.offset_mapping)
                          if (span[0] >= answer_range[0] and span[1] <= answer_range[1]) or
                             (span[0] <= answer_range[0] and span[1] >= answer_range[1]) or
                             (span[0] <= answer_range[0] and span[1] > answer_range[0])]
        token_ids = [tokenized.input_ids[idx] for idx in answer_indices]

        result_logprobs.insert(0, first_logprob)
        selected_logprobs = [lp for j, lp in enumerate(result_logprobs) if j in answer_indices]

        total_diff = 0
        for lp_item in selected_logprobs:
            lp_list = list(lp_item.items())
            if len(lp_list) == 2:
                prob_diff = (torch.exp(torch.tensor([lp_list[0][1].logprob])) -
                             torch.exp(torch.tensor([lp_list[1][1].logprob]))).item()
            else:
                prob_diff = torch.exp(torch.tensor([lp_list[0][1].logprob])).item()
            total_diff += prob_diff

        score = 0 if len(selected_logprobs) == 0 else total_diff / len(selected_logprobs)
        is_correct = (extracted.strip() == groundtruth.strip()) if groundtruth is not None else None
        answer_span_text = self.tokenizer.decode(token_ids).strip()
        return answer_span_text, score, is_correct


# Updated Decoder class which now utilizes AnswerExtractor.
class Decoder():
    def __init__(self, tokenizer: AutoTokenizer = None):
        self.extractor = AnswerExtractor(tokenizer)
        self.tokenizer = tokenizer

    def get_score(self, prompt, first_tokens, generated_outputs, groundtruth=None):
        all_paths = []
        best_candidate = None

        for i, result in enumerate(generated_outputs):
            full_reasoning = first_tokens['decoded'][i] + result.outputs[0].text
            answer_span_text, score, is_correct = self.extractor.extract(
                full_reasoning,
                first_tokens['logprobs'][i],
                result.outputs[0].logprobs,
                groundtruth
            )
            current_candidate = Path(
                reasoning_text=full_reasoning, 
                score=score,
                answer_span=answer_span_text,
                num_path=i,
                is_correct=is_correct
            )
            all_paths.append(current_candidate)
            if best_candidate is None or current_candidate.score > best_candidate.score:
                best_candidate = current_candidate

        return Result(
            question=prompt,
            paths=all_paths,
            best_path=best_candidate
        )
