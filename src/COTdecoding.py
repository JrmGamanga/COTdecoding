import re
import torch

from typing import List, Dict
from vllm import LLM, SamplingParams
from dataclasses import dataclass
from transformers import AutoTokenizer


@dataclass
class Path:
    """
    Dataclass representing a generated reasoning path.

    Attributes:
        reasoning_text (str): The complete reasoning generated by the model.
        score (float): The computed score (e.g., based on token probabilities) for this path.
        answer_span (str): The extracted answer segment from the reasoning text.
        num_path (int): The index number of this reasoning path.
        is_correct (bool): Flag indicating if the extracted answer is correct (if groundtruth is provided).
    """
    reasoning_text: str  # The complete reasoning generated by the model.
    score: float         # The computed score (e.g., based on token probabilities) for this path.
    answer_span: str     # The extracted answer segment from the reasoning text.
    num_path: int        # The index number of this reasoning path.
    is_correct: bool     # Boolean flag indicating if the extracted answer is correct (if groundtruth is provided).


@dataclass
class Result:
    """
    Dataclass representing the overall result of multiple generated reasoning paths.

    Attributes:
        question (str): The original question or prompt.
        paths (List[Path]): A list of all generated reasoning paths.
        best_path (Path): The best reasoning path based on the computed score.
    """
    question: str        # The original question or prompt.
    paths: List[Path]    # A list of all generated reasoning paths.
    best_path: Path      # The best path based on the computed score.


class GetPaths():
    """
    Class for generating multiple reasoning paths from a given prompt.

    Attributes:
        model (LLM): the LLM.
        max_new_tokens (int): Maximum number of new tokens to generate for each reasoning path.
        topk (int): The top-k tokens to consider for generating diverse paths.
        stop (List[str]): List of stop sequences to end generation.
        tokenizer: The tokenizer used by the language model.
        prompt (str): The base prompt appended to each query.
    """

    
    def __init__(self, args, 
                 max_new_tokens: int = 300, 
                 topk: int = 10, 
                 stop: List[str] = ['Q:', '\n\nQ:', '\n\nExercise'],
                 prompt: str = ''):
        self.model = LLM(model=args.model, seed=args.seed, dtype=args.dtype, max_model_len=args.max_model_len)
        self.max_new_tokens = max_new_tokens
        self.stop = stop
        self.topk = topk
        
        self.model.llm_engine.model_config.max_logprobs = self.topk + 1
        
        self.tokenizer = self.model.llm_engine.tokenizer.tokenizer
        self.prompt = prompt

    def search_cots(self, raw_prompt: str) -> List[str]:
        """
        This method formats the prompt, retrieves the top-k tokens to start the reasoning paths,
        and generates the full reasoning outputs.

        Args:
            raw_prompt (str): The original prompt to process.
        """
        formatted_query = self.format_prompt(raw_prompt)
        first_tokens = self.get_first_topk_tokens(formatted_query)
        query_prompts = [formatted_query + token for token in first_tokens['decoded']]
        path_outputs = self.generate_paths(query_prompts)
        return first_tokens, path_outputs

    @torch.inference_mode()
    def get_first_topk_tokens(self, prompt: str) -> Dict[str, List]:
        """
        Retrieve the top-k tokens from the model's output for a given prompt.

        Args:
            prompt (str): The formatted prompt string.

        Returns:
            Dict[str, List]: A dictionary containing decoded tokens, their probabilities, token IDs, and log probabilities.
        """
        sampling_settings = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=1, logprobs=self.topk, stop=self.stop)
        output_logs = self.model.generate(prompt, sampling_settings, use_tqdm=False)[0].outputs[0].logprobs[0]

        token_details = {'decoded': [], 'probs': [], 'token_id': [], 'logprobs': []}
        for token_identifier, lp_obj in output_logs.items():
            token_details['logprobs'].append({token_identifier: lp_obj})
            token_details['decoded'].append(lp_obj.decoded_token)
            token_details['probs'].append(lp_obj.logprob)
            token_details['token_id'].append(token_identifier)
        token_details['probs'] = torch.exp(torch.tensor(token_details['probs'])).tolist()
        return token_details

    @torch.inference_mode()
    def generate_paths(self, prompts: List[str]) -> Dict[int, Dict]:
        """
        Generate reasoning paths for a list of prompts.

        Args:
            prompts (List[str]): List of prompt strings for generation.

        Returns:
            Dict[int, Dict]: A dictionary mapping indices to generated output details.
        """
        generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        generated_outputs = self.model.generate(prompts, generation_params, use_tqdm=False)
        return generated_outputs

    def continue_decoding(self, prompts: List[str], mode: str = "sampling", temperature: float = 1.0, top_p: float = 0.9, num_beams: int = None) -> Dict[int, Dict]:
        """
        Continue the decoding process for given prompts using a specified decoding mode.

        This method supports sampling and beam search.

        Args:
            prompts (List[str]): List of prompt strings.
            mode (str): Decoding mode to use ("sampling" or "beam", "" is greedy by default).
            temperature (float): Sampling temperature for randomness.
            top_p (float): Top-p sampling parameter.
            num_beams (int, optional): Number of beams for beam search

        Returns:
            Dict[int, Dict]: A dictionary mapping indices to generated output details.
        """

        if mode == "sampling":
            generation_params = SamplingParams(n=1, temperature=temperature, top_p=top_p, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        elif mode == "beam":
            generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
            if num_beams is not None:
                generation_params.num_beams = num_beams
        else:
            generation_params = SamplingParams(n=1, temperature=0, top_p=1, max_tokens=self.max_new_tokens, logprobs=2, stop=self.stop)
        generated_outputs = self.model.generate(prompts, generation_params, use_tqdm=False)
        return generated_outputs

    def format_prompt(self, raw_prompt: str) -> str:
        """
        Format the raw prompt by adding a question prefix and an answer base prompt.

        Args:
            raw_prompt (str): The raw question or prompt.

        Returns:
            str: The formatted prompt string.
        """

        return f'Q:{raw_prompt}\nA:{self.prompt}'


class AnswerExtractor:
    """
    Class to extract the answer span from generated reasoning text and compute a score.

    This class uses a regular expression pattern to find numerical answers in the generated reasoning
    and computes a score based on token probabilities.
    """

    
    def __init__(self, tokenizer: AutoTokenizer, pattern: str = r'-?\d+\.?\d*'):
        self.tokenizer = tokenizer
        self.pattern = pattern

    def extract(self, full_reasoning: str, first_logprob: Dict, result_logprobs: List[Dict], groundtruth=None):
        """
        Extract the answer span and compute a score from the full reasoning text.

        This method tokenizes the reasoning text, finds the numerical answer using a regex pattern,
        calculates a score based on the token log probabilities, and optionally compares the extracted
        answer to a provided ground truth.

        Args:
            full_reasoning (str): The complete reasoning text generated by the model.
            first_logprob (Dict): The log probability details of the first token.
            result_logprobs (List[Dict]): Log probabilities for subsequent tokens.
            groundtruth (str, optional): The ground truth answer for comparison.

        Returns:
            Tuple[str, float, bool]: A tuple containing the extracted answer span text, computed score, and a flag indicating correctness.
        """

        tokenized = self.tokenizer(full_reasoning, return_offsets_mapping=True)
        answer_matches = re.findall(self.pattern, full_reasoning)
        if answer_matches:
            extracted = answer_matches[-1]
        else:
            return 'Did not respond explicitly', 0, False

        answer_range = (full_reasoning.rfind(extracted), full_reasoning.rfind(extracted) + len(extracted))
        answer_indices = [j for j, span in enumerate(tokenized.offset_mapping)
                          if (span[0] >= answer_range[0] and span[1] <= answer_range[1]) or
                             (span[0] <= answer_range[0] and span[1] >= answer_range[1]) or
                             (span[0] <= answer_range[0] and span[1] > answer_range[0])]
        token_ids = [tokenized.input_ids[idx] for idx in answer_indices]

        result_logprobs.insert(0, first_logprob)
        selected_logprobs = [lp for j, lp in enumerate(result_logprobs) if j in answer_indices]

        total_diff = 0
        for lp_item in selected_logprobs:
            lp_list = list(lp_item.items())
            if len(lp_list) == 2:
                prob_diff = (torch.exp(torch.tensor([lp_list[0][1].logprob])) -
                             torch.exp(torch.tensor([lp_list[1][1].logprob]))).item()
            else:
                prob_diff = torch.exp(torch.tensor([lp_list[0][1].logprob])).item()
            total_diff += prob_diff

        score = 0 if len(selected_logprobs) == 0 else total_diff / len(selected_logprobs)
        is_correct = (extracted.strip() == groundtruth.strip()) if groundtruth is not None else None
        answer_span_text = self.tokenizer.decode(token_ids).strip()
        return answer_span_text, score, is_correct


class Decoder():
    """
    Class for decoding generated reasoning paths and selecting the best one based on computed scores.
    """
    def __init__(self, tokenizer: AutoTokenizer = None):
        self.extractor = AnswerExtractor(tokenizer)
        self.tokenizer = tokenizer

    def get_score(self, prompt, first_tokens, generated_outputs, groundtruth=None):
        """
        Compute scores for generated reasoning paths and select the best path.

        This method processes the generated outputs by concatenating initial tokens with the reasoning text,
        extracts the answer span and score for each path using an AnswerExtractor, and returns the overall result.

        Args:
            prompt (str): The original prompt.
            first_tokens (Dict): Details of the first tokens from the model.
            generated_outputs (Dict): Generated outputs from the model for multiple paths.
            groundtruth (str, optional): The ground truth answer for correctness comparison.

        Returns:
            Result: A dataclass containing the question, a list of all reasoning paths, and the best reasoning path.
        """

        all_paths = []
        best_candidate = None

        for i, result in enumerate(generated_outputs):
            full_reasoning = first_tokens['decoded'][i] + result.outputs[0].text
            answer_span_text, score, is_correct = self.extractor.extract(
                full_reasoning,
                first_tokens['logprobs'][i],
                result.outputs[0].logprobs,
                groundtruth
            )
            current_candidate = Path(
                reasoning_text=full_reasoning, 
                score=score,
                answer_span=answer_span_text,
                num_path=i,
                is_correct=is_correct
            )
            all_paths.append(current_candidate)
            if best_candidate is None or current_candidate.score > best_candidate.score:
                best_candidate = current_candidate

        return Result(
            question=prompt,
            paths=all_paths,
            best_path=best_candidate
        )
